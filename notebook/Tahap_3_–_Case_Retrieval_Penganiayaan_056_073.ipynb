{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Tahap 3 Disusun Oleh** 👨‍🎓✒️\n",
        ">**Nama: Fitra Romeo Winky**\n",
        "\n",
        ">**NIM: 202210370311056**\n",
        "\n",
        ">**Nama: Muhammad Aunul Hakim**\n",
        "\n",
        ">**NIM: 202210370311073**\n",
        "\n",
        ">**Kelas: C**\n",
        "\n",
        ">**Mata Kuliah: Penalaran Komputer**\n",
        "\n",
        ">**Jurusan: Informatika**"
      ],
      "metadata": {
        "id": "gVWL8NCsd1jy"
      },
      "id": "gVWL8NCsd1jy"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOlgBVTEaYqZ",
        "outputId": "2576f1c4-b026-45d5-bd4a-dfbee847ab23"
      },
      "id": "yOlgBVTEaYqZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tahap 3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class CaseRetrievalSystem:\n",
        "    def __init__(self, dataset_path):\n",
        "        \"\"\"\n",
        "        Initialize Case Retrieval System\n",
        "\n",
        "        Args:\n",
        "            dataset_path (str): Path to the cases.csv dataset\n",
        "        \"\"\"\n",
        "        self.dataset_path = dataset_path\n",
        "        self.df = None\n",
        "        self.tfidf_vectorizer = None\n",
        "        self.tfidf_matrix = None\n",
        "        self.bert_tokenizer = None\n",
        "        self.bert_model = None\n",
        "        self.bert_embeddings = None\n",
        "        self.ml_model = None\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and preprocess the dataset\"\"\"\n",
        "        print(\"Loading dataset...\")\n",
        "        self.df = pd.read_csv(self.dataset_path)\n",
        "        print(f\"Dataset loaded: {len(self.df)} cases\")\n",
        "\n",
        "        # Updated text columns based on new attributes\n",
        "        text_columns = ['ringkasan_fakta', 'argumen_hukum', 'pihak']\n",
        "\n",
        "        # Create combined text from available columns\n",
        "        combined_parts = []\n",
        "        for col in text_columns:\n",
        "            if col in self.df.columns:\n",
        "                combined_parts.append(self.df[col].fillna('').astype(str))\n",
        "\n",
        "        if combined_parts:\n",
        "            self.df['combined_text'] = combined_parts[0]\n",
        "            for part in combined_parts[1:]:\n",
        "                self.df['combined_text'] = self.df['combined_text'] + ' ' + part\n",
        "        else:\n",
        "            self.df['combined_text'] = ''\n",
        "\n",
        "        # Use text_full if available, otherwise use combined_text\n",
        "        if 'text_full' in self.df.columns:\n",
        "            self.df['retrieval_text'] = self.df['text_full'].fillna(self.df['combined_text'])\n",
        "        else:\n",
        "            self.df['retrieval_text'] = self.df['combined_text']\n",
        "\n",
        "        # Ensure case_id exists (it's already in the new attributes)\n",
        "        if 'case_id' not in self.df.columns:\n",
        "            self.df['case_id'] = ['case_' + str(i).zfill(4) for i in range(len(self.df))]\n",
        "\n",
        "        print(\"Data preprocessing completed\")\n",
        "        print(f\"Available columns: {list(self.df.columns)}\")\n",
        "        return self.df\n",
        "\n",
        "    def create_tfidf_representation(self, max_features=5000):\n",
        "        \"\"\"\n",
        "        Create TF-IDF representation of the cases\n",
        "\n",
        "        Args:\n",
        "            max_features (int): Maximum number of features for TF-IDF\n",
        "        \"\"\"\n",
        "        print(\"Creating TF-IDF representation...\")\n",
        "\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(\n",
        "            max_features=max_features,\n",
        "            stop_words=None,  # You can add Indonesian stop words here\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=2,\n",
        "            max_df=0.8\n",
        "        )\n",
        "\n",
        "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.df['retrieval_text'])\n",
        "        print(f\"TF-IDF matrix shape: {self.tfidf_matrix.shape}\")\n",
        "        return self.tfidf_matrix\n",
        "\n",
        "    def create_bert_representation(self, model_name='indobenchmark/indobert-base-p1'):\n",
        "        \"\"\"\n",
        "        Create BERT embeddings for the cases\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Pre-trained BERT model name\n",
        "        \"\"\"\n",
        "        print(f\"Loading BERT model: {model_name}\")\n",
        "\n",
        "        try:\n",
        "            self.bert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.bert_model = AutoModel.from_pretrained(model_name)\n",
        "            self.bert_model.eval()\n",
        "        except:\n",
        "            print(\"Failed to load IndoBERT, using multilingual BERT instead...\")\n",
        "            model_name = 'bert-base-multilingual-cased'\n",
        "            self.bert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.bert_model = AutoModel.from_pretrained(model_name)\n",
        "            self.bert_model.eval()\n",
        "\n",
        "        print(\"Creating BERT embeddings...\")\n",
        "        embeddings = []\n",
        "\n",
        "        for idx, text in enumerate(self.df['retrieval_text']):\n",
        "            if idx % 10 == 0:\n",
        "                print(f\"Processing {idx+1}/{len(self.df)} cases...\")\n",
        "\n",
        "            # Truncate text to avoid memory issues\n",
        "            text = str(text)[:512]\n",
        "\n",
        "            # Tokenize and encode\n",
        "            inputs = self.bert_tokenizer(\n",
        "                text,\n",
        "                return_tensors='pt',\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert_model(**inputs)\n",
        "                # Use CLS token embedding\n",
        "                embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
        "                embeddings.append(embedding)\n",
        "\n",
        "        self.bert_embeddings = np.array(embeddings)\n",
        "        print(f\"BERT embeddings shape: {self.bert_embeddings.shape}\")\n",
        "        return self.bert_embeddings\n",
        "\n",
        "    def split_data(self, test_size=0.3, random_state=42):\n",
        "        \"\"\"\n",
        "        Split data for training and testing\n",
        "\n",
        "        Args:\n",
        "            test_size (float): Proportion of test data\n",
        "            random_state (int): Random seed\n",
        "        \"\"\"\n",
        "        print(f\"Splitting data with ratio {1-test_size:.1f}:{test_size:.1f}\")\n",
        "\n",
        "        # Use jenis_perkara as target for classification\n",
        "        y = self.df['jenis_perkara'].fillna('unknown')\n",
        "\n",
        "        # Check class distribution\n",
        "        class_counts = y.value_counts()\n",
        "        print(f\"Class distribution:\\n{class_counts}\")\n",
        "\n",
        "        # Use .values to get numpy array, then check all elements\n",
        "        class_count_values = class_counts.values  # This is already a numpy array\n",
        "        can_stratify = all(count >= 2 for count in class_count_values)\n",
        "\n",
        "        # Check if stratification is possible\n",
        "        if can_stratify and len(y.unique()) > 1:\n",
        "            print(\"Using stratified split\")\n",
        "            stratify_param = y\n",
        "        else:\n",
        "            print(\"Using random split (stratification not possible due to class imbalance)\")\n",
        "            stratify_param = None\n",
        "\n",
        "        # Split using TF-IDF features\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.tfidf_matrix.toarray(),\n",
        "            y,\n",
        "            test_size=test_size,\n",
        "            random_state=random_state,\n",
        "            stratify=stratify_param\n",
        "        )\n",
        "\n",
        "        print(f\"Training set: {self.X_train.shape[0]} cases\")\n",
        "        print(f\"Test set: {self.X_test.shape[0]} cases\")\n",
        "        print(f\"Training set class distribution:\\n{pd.Series(self.y_train).value_counts()}\")\n",
        "        print(f\"Test set class distribution:\\n{pd.Series(self.y_test).value_counts()}\")\n",
        "\n",
        "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
        "\n",
        "    def train_ml_model(self, model_type='svm'):\n",
        "        \"\"\"\n",
        "        Train machine learning model for retrieval\n",
        "\n",
        "        Args:\n",
        "            model_type (str): 'svm' or 'naive_bayes'\n",
        "        \"\"\"\n",
        "        print(f\"Training {model_type.upper()} model...\")\n",
        "\n",
        "        # Check if we have enough classes and samples for training\n",
        "        unique_classes = len(set(self.y_train))\n",
        "        if unique_classes < 2:\n",
        "            print(\"Warning: Only one class in training data. Skipping ML model training.\")\n",
        "            return None\n",
        "\n",
        "        if model_type.lower() == 'svm':\n",
        "            # Use different kernel for small datasets\n",
        "            if len(self.y_train) < 100:\n",
        "                self.ml_model = SVC(kernel='rbf', probability=True, random_state=42, C=1.0)\n",
        "            else:\n",
        "                self.ml_model = SVC(kernel='linear', probability=True, random_state=42)\n",
        "        elif model_type.lower() == 'naive_bayes':\n",
        "            self.ml_model = MultinomialNB(alpha=1.0)\n",
        "        else:\n",
        "            raise ValueError(\"model_type must be 'svm' or 'naive_bayes'\")\n",
        "\n",
        "        try:\n",
        "            self.ml_model.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Evaluate model\n",
        "            train_pred = self.ml_model.predict(self.X_train)\n",
        "            test_pred = self.ml_model.predict(self.X_test)\n",
        "\n",
        "            print(f\"Training Accuracy: {accuracy_score(self.y_train, train_pred):.4f}\")\n",
        "            print(f\"Test Accuracy: {accuracy_score(self.y_test, test_pred):.4f}\")\n",
        "\n",
        "            # Show detailed classification report for test set\n",
        "            print(\"\\nDetailed Classification Report:\")\n",
        "            print(classification_report(self.y_test, test_pred, zero_division=0))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training model: {e}\")\n",
        "            print(\"Continuing without ML model...\")\n",
        "            self.ml_model = None\n",
        "\n",
        "        return self.ml_model\n",
        "\n",
        "    def retrieve_tfidf(self, query_text, top_k=5):\n",
        "        \"\"\"\n",
        "        Retrieve similar cases using TF-IDF and cosine similarity\n",
        "\n",
        "        Args:\n",
        "            query_text (str): Query text\n",
        "            top_k (int): Number of top similar cases to return\n",
        "\n",
        "        Returns:\n",
        "            list: List of tuples (case_id, similarity_score, case_info)\n",
        "        \"\"\"\n",
        "        # Transform query to TF-IDF vector\n",
        "        query_vector = self.tfidf_vectorizer.transform([query_text])\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
        "\n",
        "        # Get top-k most similar cases\n",
        "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            case_info = {\n",
        "                'case_id': self.df.iloc[idx]['case_id'],\n",
        "                'no_perkara': self.df.iloc[idx].get('no_perkara', 'N/A'),\n",
        "                'tanggal': self.df.iloc[idx].get('tanggal', 'N/A'),\n",
        "                'jenis_perkara': self.df.iloc[idx]['jenis_perkara'],\n",
        "                'pasal': self.df.iloc[idx]['pasal'],\n",
        "                'pihak': self.df.iloc[idx].get('pihak', 'N/A'),\n",
        "                'ringkasan_fakta': self.df.iloc[idx]['ringkasan_fakta'],\n",
        "                'argumen_hukum': self.df.iloc[idx].get('argumen_hukum', 'N/A'),\n",
        "                'length_kata': self.df.iloc[idx].get('length_kata', 'N/A'),\n",
        "                'jumlah_pasal': self.df.iloc[idx].get('jumlah_pasal', 'N/A'),\n",
        "                'mengandung_pidana': self.df.iloc[idx].get('mengandung_pidana', 'N/A'),\n",
        "                'mengandung_penganiayaan': self.df.iloc[idx].get('mengandung_penganiayaan', 'N/A'),\n",
        "                'similarity_score': float(similarities[idx])\n",
        "            }\n",
        "            results.append((self.df.iloc[idx]['case_id'], similarities[idx], case_info))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def retrieve_bert(self, query_text, top_k=5):\n",
        "        \"\"\"\n",
        "        Retrieve similar cases using BERT embeddings\n",
        "\n",
        "        Args:\n",
        "            query_text (str): Query text\n",
        "            top_k (int): Number of top similar cases to return\n",
        "\n",
        "        Returns:\n",
        "            list: List of tuples (case_id, similarity_score, case_info)\n",
        "        \"\"\"\n",
        "        # Get BERT embedding for query\n",
        "        query_text = str(query_text)[:512]\n",
        "        inputs = self.bert_tokenizer(\n",
        "            query_text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.bert_model(**inputs)\n",
        "            query_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
        "\n",
        "        # Calculate cosine similarity with all case embeddings\n",
        "        similarities = cosine_similarity([query_embedding], self.bert_embeddings).flatten()\n",
        "\n",
        "        # Get top-k most similar cases\n",
        "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            case_info = {\n",
        "                'case_id': self.df.iloc[idx]['case_id'],\n",
        "                'no_perkara': self.df.iloc[idx].get('no_perkara', 'N/A'),\n",
        "                'tanggal': self.df.iloc[idx].get('tanggal', 'N/A'),\n",
        "                'jenis_perkara': self.df.iloc[idx]['jenis_perkara'],\n",
        "                'pasal': self.df.iloc[idx]['pasal'],\n",
        "                'pihak': self.df.iloc[idx].get('pihak', 'N/A'),\n",
        "                'ringkasan_fakta': self.df.iloc[idx]['ringkasan_fakta'],\n",
        "                'argumen_hukum': self.df.iloc[idx].get('argumen_hukum', 'N/A'),\n",
        "                'length_kata': self.df.iloc[idx].get('length_kata', 'N/A'),\n",
        "                'jumlah_pasal': self.df.iloc[idx].get('jumlah_pasal', 'N/A'),\n",
        "                'mengandung_pidana': self.df.iloc[idx].get('mengandung_pidana', 'N/A'),\n",
        "                'mengandung_penganiayaan': self.df.iloc[idx].get('mengandung_penganiayaan', 'N/A'),\n",
        "                'similarity_score': float(similarities[idx])\n",
        "            }\n",
        "            results.append((self.df.iloc[idx]['case_id'], similarities[idx], case_info))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def retrieve(self, query_text, method='tfidf', top_k=5):\n",
        "        \"\"\"\n",
        "        Main retrieval function\n",
        "\n",
        "        Args:\n",
        "            query_text (str): Query text\n",
        "            method (str): 'tfidf' or 'bert'\n",
        "            top_k (int): Number of top similar cases to return\n",
        "\n",
        "        Returns:\n",
        "            list: List of retrieved cases\n",
        "        \"\"\"\n",
        "        if method.lower() == 'tfidf':\n",
        "            return self.retrieve_tfidf(query_text, top_k)\n",
        "        elif method.lower() == 'bert':\n",
        "            return self.retrieve_bert(query_text, top_k)\n",
        "        else:\n",
        "            raise ValueError(\"method must be 'tfidf' or 'bert'\")\n",
        "\n",
        "    def create_test_queries(self, num_queries=10):\n",
        "        \"\"\"\n",
        "        Create test queries from existing cases\n",
        "\n",
        "        Args:\n",
        "            num_queries (int): Number of test queries to create\n",
        "\n",
        "        Returns:\n",
        "            list: List of test queries with ground truth\n",
        "        \"\"\"\n",
        "        print(f\"Creating {num_queries} test queries...\")\n",
        "\n",
        "        # Ensure we don't sample more queries than we have cases\n",
        "        actual_num_queries = min(num_queries, len(self.df))\n",
        "\n",
        "        # Sample random cases for queries\n",
        "        sample_indices = np.random.choice(len(self.df), actual_num_queries, replace=False)\n",
        "\n",
        "        queries = []\n",
        "        for idx in sample_indices:\n",
        "            case = self.df.iloc[idx]\n",
        "\n",
        "            # Create query from ringkasan_fakta or first part of text\n",
        "            if pd.notna(case['ringkasan_fakta']) and str(case['ringkasan_fakta']).strip():\n",
        "                query_text = str(case['ringkasan_fakta'])\n",
        "            elif pd.notna(case.get('argumen_hukum')) and str(case.get('argumen_hukum', '')).strip():\n",
        "                query_text = str(case['argumen_hukum'])[:200]\n",
        "            else:\n",
        "                query_text = str(case['retrieval_text'])[:200]\n",
        "\n",
        "            query = {\n",
        "                'query_id': f'query_{len(queries)+1:03d}',\n",
        "                'query_text': query_text.strip(),\n",
        "                'ground_truth_case_id': str(case['case_id']),\n",
        "                'expected_jenis_perkara': str(case['jenis_perkara']) if pd.notna(case['jenis_perkara']) else 'unknown',\n",
        "                'expected_pasal': str(case['pasal']) if pd.notna(case['pasal']) else 'unknown',\n",
        "                'expected_no_perkara': str(case.get('no_perkara', 'unknown')),\n",
        "                'source_index': int(idx)\n",
        "            }\n",
        "            queries.append(query)\n",
        "\n",
        "        print(f\"Created {len(queries)} test queries\")\n",
        "        return queries\n",
        "\n",
        "    def evaluate_retrieval(self, queries, method='tfidf', top_k=5):\n",
        "        \"\"\"\n",
        "        Evaluate retrieval performance\n",
        "\n",
        "        Args:\n",
        "            queries (list): List of test queries\n",
        "            method (str): Retrieval method\n",
        "            top_k (int): Number of top results to consider\n",
        "\n",
        "        Returns:\n",
        "            dict: Evaluation metrics\n",
        "        \"\"\"\n",
        "        print(f\"Evaluating retrieval with {method} method...\")\n",
        "\n",
        "        hits_at_1 = 0\n",
        "        hits_at_k = 0\n",
        "        mrr_scores = []\n",
        "\n",
        "        for query in queries:\n",
        "            results = self.retrieve(query['query_text'], method=method, top_k=top_k)\n",
        "\n",
        "            # Check if ground truth is in top-1\n",
        "            if results[0][0] == query['ground_truth_case_id']:\n",
        "                hits_at_1 += 1\n",
        "\n",
        "            # Check if ground truth is in top-k\n",
        "            retrieved_ids = [result[0] for result in results]\n",
        "            if query['ground_truth_case_id'] in retrieved_ids:\n",
        "                hits_at_k += 1\n",
        "                # Calculate reciprocal rank\n",
        "                rank = retrieved_ids.index(query['ground_truth_case_id']) + 1\n",
        "                mrr_scores.append(1.0 / rank)\n",
        "            else:\n",
        "                mrr_scores.append(0.0)\n",
        "\n",
        "        metrics = {\n",
        "            'hits_at_1': hits_at_1 / len(queries),\n",
        "            f'hits_at_{top_k}': hits_at_k / len(queries),\n",
        "            'mrr': np.mean(mrr_scores),\n",
        "            'total_queries': len(queries)\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # Initialize system\n",
        "    dataset_path = '/content/drive/MyDrive/SEMESTER 6/Penalaran Komputer/UAS_Penalaran Komputer/data/processed/cases.csv'\n",
        "    retrieval_system = CaseRetrievalSystem(dataset_path)\n",
        "\n",
        "    try:\n",
        "        # Load and preprocess data\n",
        "        df = retrieval_system.load_data()\n",
        "        print(f\"Data shape: {df.shape}\")\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "        # Show sample of jenis_perkara distribution\n",
        "        print(f\"\\nJenis Perkara distribution:\")\n",
        "        perkara_counts = df['jenis_perkara'].value_counts()\n",
        "        print(perkara_counts.head(10))\n",
        "\n",
        "        # Show additional statistics for new columns\n",
        "        if 'mengandung_pidana' in df.columns:\n",
        "            print(f\"\\nMengandung Pidana distribution:\")\n",
        "            print(df['mengandung_pidana'].value_counts())\n",
        "\n",
        "        if 'mengandung_penganiayaan' in df.columns:\n",
        "            print(f\"\\nMengandung Penganiayaan distribution:\")\n",
        "            print(df['mengandung_penganiayaan'].value_counts())\n",
        "\n",
        "        # Create representations\n",
        "        print(\"\\n=== Creating TF-IDF Representation ===\")\n",
        "        tfidf_matrix = retrieval_system.create_tfidf_representation()\n",
        "\n",
        "        print(\"\\n=== Creating BERT Representation ===\")\n",
        "        bert_embeddings = retrieval_system.create_bert_representation()\n",
        "\n",
        "        # Split data\n",
        "        print(\"\\n=== Splitting Data ===\")\n",
        "        X_train, X_test, y_train, y_test = retrieval_system.split_data(test_size=0.3)\n",
        "\n",
        "        # Train ML model\n",
        "        print(\"\\n=== Training ML Model ===\")\n",
        "        ml_model = retrieval_system.train_ml_model(model_type='svm')\n",
        "\n",
        "        # Create test queries\n",
        "        print(\"\\n=== Creating Test Queries ===\")\n",
        "        test_queries = retrieval_system.create_test_queries(num_queries=min(10, len(df)))\n",
        "\n",
        "        # Create directory and save test queries\n",
        "        eval_dir = '/content/drive/MyDrive/SEMESTER 6/Penalaran Komputer/UAS_Penalaran Komputer/data/eval'\n",
        "        os.makedirs(eval_dir, exist_ok=True)\n",
        "\n",
        "        queries_file = os.path.join(eval_dir, 'queries.json')\n",
        "        with open(queries_file, 'w', encoding='utf-8') as f:\n",
        "            # Convert any remaining numpy types to native Python types\n",
        "            serializable_queries = []\n",
        "            for query in test_queries:\n",
        "                serializable_query = {}\n",
        "                for key, value in query.items():\n",
        "                    if isinstance(value, (np.integer, np.int64, np.int32)):\n",
        "                        serializable_query[key] = int(value)\n",
        "                    elif isinstance(value, (np.floating, np.float64, np.float32)):\n",
        "                        serializable_query[key] = float(value)\n",
        "                    elif isinstance(value, np.ndarray):\n",
        "                        serializable_query[key] = value.tolist()\n",
        "                    else:\n",
        "                        serializable_query[key] = value\n",
        "                serializable_queries.append(serializable_query)\n",
        "\n",
        "            json.dump(serializable_queries, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"Test queries saved to {queries_file}\")\n",
        "\n",
        "        # Evaluate retrieval\n",
        "        if len(test_queries) > 0:\n",
        "            print(\"\\n=== Evaluating TF-IDF Retrieval ===\")\n",
        "            tfidf_metrics = retrieval_system.evaluate_retrieval(test_queries, method='tfidf', top_k=5)\n",
        "            print(f\"TF-IDF Metrics: {tfidf_metrics}\")\n",
        "\n",
        "            print(\"\\n=== Evaluating BERT Retrieval ===\")\n",
        "            bert_metrics = retrieval_system.evaluate_retrieval(test_queries, method='bert', top_k=5)\n",
        "            print(f\"BERT Metrics: {bert_metrics}\")\n",
        "\n",
        "        # Test retrieval function with sample queries\n",
        "        print(\"\\n=== Testing Retrieval Function ===\")\n",
        "        sample_queries = [\n",
        "            \"kasus perceraian dengan harta gono gini\",\n",
        "            \"sengketa kontrak kerja\",\n",
        "            \"gugatan wanprestasi pembayaran\",\n",
        "            \"pembatalan perjanjian jual beli\"\n",
        "        ]\n",
        "\n",
        "        for sample_query in sample_queries[:2]:  # Test first 2 queries\n",
        "            print(f\"\\n\" + \"=\"*50)\n",
        "            print(f\"Query: {sample_query}\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            try:\n",
        "                print(\"\\nTF-IDF Results:\")\n",
        "                tfidf_results = retrieval_system.retrieve(sample_query, method='tfidf', top_k=3)\n",
        "                for i, (case_id, score, info) in enumerate(tfidf_results, 1):\n",
        "                    print(f\"{i}. Case ID: {case_id}, Score: {score:.4f}\")\n",
        "                    print(f\"   No Perkara: {info['no_perkara']}\")\n",
        "                    print(f\"   Tanggal: {info['tanggal']}\")\n",
        "                    print(f\"   Jenis Perkara: {info['jenis_perkara']}\")\n",
        "                    print(f\"   Pasal: {info['pasal']}\")\n",
        "                    print(f\"   Mengandung Pidana: {info['mengandung_pidana']}\")\n",
        "                    print(f\"   Mengandung Penganiayaan: {info['mengandung_penganiayaan']}\")\n",
        "                    print(f\"   Ringkasan: {str(info['ringkasan_fakta'])[:100]}...\")\n",
        "                    print()\n",
        "            except Exception as e:\n",
        "                print(f\"Error in TF-IDF retrieval: {e}\")\n",
        "\n",
        "            try:\n",
        "                print(\"BERT Results:\")\n",
        "                bert_results = retrieval_system.retrieve(sample_query, method='bert', top_k=3)\n",
        "                for i, (case_id, score, info) in enumerate(bert_results, 1):\n",
        "                    print(f\"{i}. Case ID: {case_id}, Score: {score:.4f}\")\n",
        "                    print(f\"   No Perkara: {info['no_perkara']}\")\n",
        "                    print(f\"   Tanggal: {info['tanggal']}\")\n",
        "                    print(f\"   Jenis Perkara: {info['jenis_perkara']}\")\n",
        "                    print(f\"   Pasal: {info['pasal']}\")\n",
        "                    print(f\"   Mengandung Pidana: {info['mengandung_pidana']}\")\n",
        "                    print(f\"   Mengandung Penganiayaan: {info['mengandung_penganiayaan']}\")\n",
        "                    print(f\"   Ringkasan: {str(info['ringkasan_fakta'])[:100]}...\")\n",
        "                    print()\n",
        "            except Exception as e:\n",
        "                print(f\"Error in BERT retrieval: {e}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"=== Case Retrieval System Setup Complete ===\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Save system summary\n",
        "        summary = {\n",
        "            \"dataset_info\": {\n",
        "                \"total_cases\": len(df),\n",
        "                \"tfidf_features\": tfidf_matrix.shape[1],\n",
        "                \"bert_embedding_dim\": bert_embeddings.shape[1],\n",
        "                \"unique_jenis_perkara\": len(df['jenis_perkara'].unique()),\n",
        "                \"columns\": list(df.columns)\n",
        "            },\n",
        "            \"split_info\": {\n",
        "                \"train_size\": len(X_train),\n",
        "                \"test_size\": len(X_test),\n",
        "                \"test_ratio\": len(X_test) / (len(X_train) + len(X_test))\n",
        "            },\n",
        "            \"model_info\": {\n",
        "                \"ml_model_trained\": ml_model is not None,\n",
        "                \"model_type\": type(ml_model).__name__ if ml_model else None\n",
        "            },\n",
        "            \"evaluation\": {\n",
        "                \"num_test_queries\": len(test_queries),\n",
        "                \"tfidf_metrics\": tfidf_metrics if 'tfidf_metrics' in locals() else None,\n",
        "                \"bert_metrics\": bert_metrics if 'bert_metrics' in locals() else None\n",
        "            }\n",
        "        }\n",
        "\n",
        "        summary_file = os.path.join(eval_dir, 'system_summary.json')\n",
        "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(summary, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "        print(f\"System summary saved to {summary_file}\")\n",
        "\n",
        "        return retrieval_system\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the main function\n",
        "    system = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYgOStn0lTgO",
        "outputId": "d8a18dc6-5a3d-4201-ac3c-f2c7fed2eba8"
      },
      "id": "GYgOStn0lTgO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Dataset loaded: 31 cases\n",
            "Data preprocessing completed\n",
            "Available columns: ['case_id', 'no_perkara', 'tanggal', 'jenis_perkara', 'pasal', 'pihak', 'ringkasan_fakta', 'argumen_hukum', 'length_kata', 'jumlah_pasal', 'mengandung_pidana', 'mengandung_penganiayaan', 'text_full', 'combined_text', 'retrieval_text']\n",
            "Data shape: (31, 15)\n",
            "Columns: ['case_id', 'no_perkara', 'tanggal', 'jenis_perkara', 'pasal', 'pihak', 'ringkasan_fakta', 'argumen_hukum', 'length_kata', 'jumlah_pasal', 'mengandung_pidana', 'mengandung_penganiayaan', 'text_full', 'combined_text', 'retrieval_text']\n",
            "\n",
            "Jenis Perkara distribution:\n",
            "jenis_perkara\n",
            "Pidana Umum \\n Pidana Umum  Penganiayaan    24\n",
            "Perdata Agama                                3\n",
            "Pidana Umum                                  2\n",
            "Pidana Militer                               1\n",
            "Pidana Umum  Penganiayaan                    1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Mengandung Pidana distribution:\n",
            "mengandung_pidana\n",
            "True     27\n",
            "False     4\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Mengandung Penganiayaan distribution:\n",
            "mengandung_penganiayaan\n",
            "True     27\n",
            "False     4\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== Creating TF-IDF Representation ===\n",
            "Creating TF-IDF representation...\n",
            "TF-IDF matrix shape: (31, 5000)\n",
            "\n",
            "=== Creating BERT Representation ===\n",
            "Loading BERT model: indobenchmark/indobert-base-p1\n",
            "Creating BERT embeddings...\n",
            "Processing 1/31 cases...\n",
            "Processing 11/31 cases...\n",
            "Processing 21/31 cases...\n",
            "Processing 31/31 cases...\n",
            "BERT embeddings shape: (31, 768)\n",
            "\n",
            "=== Splitting Data ===\n",
            "Splitting data with ratio 0.7:0.3\n",
            "Class distribution:\n",
            "jenis_perkara\n",
            "Pidana Umum \\n Pidana Umum  Penganiayaan    24\n",
            "Perdata Agama                                3\n",
            "Pidana Umum                                  2\n",
            "Pidana Militer                               1\n",
            "Pidana Umum  Penganiayaan                    1\n",
            "Name: count, dtype: int64\n",
            "Using random split (stratification not possible due to class imbalance)\n",
            "Training set: 21 cases\n",
            "Test set: 10 cases\n",
            "Training set class distribution:\n",
            "jenis_perkara\n",
            "Pidana Umum \\n Pidana Umum  Penganiayaan    15\n",
            "Perdata Agama                                3\n",
            "Pidana Umum                                  2\n",
            "Pidana Umum  Penganiayaan                    1\n",
            "Name: count, dtype: int64\n",
            "Test set class distribution:\n",
            "jenis_perkara\n",
            "Pidana Umum \\n Pidana Umum  Penganiayaan    9\n",
            "Pidana Militer                              1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== Training ML Model ===\n",
            "Training SVM model...\n",
            "Training Accuracy: 0.8571\n",
            "Test Accuracy: 0.9000\n",
            "\n",
            "Detailed Classification Report:\n",
            "                                         precision    recall  f1-score   support\n",
            "\n",
            "                         Pidana Militer       0.00      0.00      0.00         1\n",
            "Pidana Umum \n",
            " Pidana Umum  Penganiayaan       0.90      1.00      0.95         9\n",
            "\n",
            "                               accuracy                           0.90        10\n",
            "                              macro avg       0.45      0.50      0.47        10\n",
            "                           weighted avg       0.81      0.90      0.85        10\n",
            "\n",
            "\n",
            "=== Creating Test Queries ===\n",
            "Creating 10 test queries...\n",
            "Created 10 test queries\n",
            "Test queries saved to /content/drive/MyDrive/SEMESTER 6/Penalaran Komputer/UAS_Penalaran Komputer/data/eval/queries.json\n",
            "\n",
            "=== Evaluating TF-IDF Retrieval ===\n",
            "Evaluating retrieval with tfidf method...\n",
            "TF-IDF Metrics: {'hits_at_1': 0.0, 'hits_at_5': 0.0, 'mrr': np.float64(0.0), 'total_queries': 10}\n",
            "\n",
            "=== Evaluating BERT Retrieval ===\n",
            "Evaluating retrieval with bert method...\n",
            "BERT Metrics: {'hits_at_1': 0.0, 'hits_at_5': 0.0, 'mrr': np.float64(0.0), 'total_queries': 10}\n",
            "\n",
            "=== Testing Retrieval Function ===\n",
            "\n",
            "==================================================\n",
            "Query: kasus perceraian dengan harta gono gini\n",
            "==================================================\n",
            "\n",
            "TF-IDF Results:\n",
            "1. Case ID: 10, Score: 0.0583\n",
            "   No Perkara: 735/Pid.B/2025/PN Lbp\n",
            "   Tanggal: 7 Mei 2025\n",
            "   Jenis Perkara: Pidana Umum \n",
            " Pidana Umum  Penganiayaan\n",
            "   Pasal: —\n",
            "   Mengandung Pidana: True\n",
            "   Mengandung Penganiayaan: True\n",
            "   Ringkasan: Direktori Putusan Mahkamah Agung Republik Indonesia\n",
            "putusan.mahkamahagung.go.id\n",
            "\n",
            "Mahkamah Agung Repu...\n",
            "\n",
            "2. Case ID: 25, Score: 0.0225\n",
            "   No Perkara: 38/Pid.B/2025/PN Bla\n",
            "   Tanggal: 6 Mei 2025\n",
            "   Jenis Perkara: Pidana Umum \n",
            " Pidana Umum  Penganiayaan\n",
            "   Pasal: —\n",
            "   Mengandung Pidana: True\n",
            "   Mengandung Penganiayaan: True\n",
            "   Ringkasan: Direktori Putusan Mahkamah Agung Republik Indonesia\n",
            "putusan.mahkamahagung.go.id\n",
            "\n",
            "Mahkamah Agung Repu...\n",
            "\n",
            "3. Case ID: 38, Score: 0.0000\n",
            "   No Perkara: 55/Pid.B/2025/PN Ksn\n",
            "   Tanggal: 8 Mei 2025\n",
            "   Jenis Perkara: Pidana Umum \n",
            " Pidana Umum  Penganiayaan\n",
            "   Pasal: —\n",
            "   Mengandung Pidana: True\n",
            "   Mengandung Penganiayaan: True\n",
            "   Ringkasan: Direktori Putusan Mahkamah Agung Republik Indonesia\n",
            "putusan.mahkamahagung.go.id\n",
            "\n",
            "Mahkamah Agung Repu...\n",
            "\n",
            "BERT Results:\n",
            "1. Case ID: 7, Score: 0.7064\n",
            "   No Perkara: 46/Pid.B/2025/PN Bla\n",
            "   Tanggal: 19 Mei 2025\n",
            "   Jenis Perkara: Pidana Umum \n",
            " Pidana Umum  Penganiayaan\n",
            "   Pasal: —\n",
            "   Mengandung Pidana: True\n",
            "   Mengandung Penganiayaan: True\n",
            "   Ringkasan: Direktori Putusan Mahkamah Agung Republik Indonesia\n",
            "putusan.mahkamahagung.go.id\n",
            "\n",
            "Mahkamah Agung Repu...\n",
            "\n",
            "2. Case ID: 25, Score: 0.7061\n",
            "   No Perkara: 38/Pid.B/2025/PN Bla\n",
            "   Tanggal: 6 Mei 2025\n",
            "   Jenis Perkara: Pidana Umum \n",
            " Pidana Umum  Penganiayaan\n",
            "   Pasal: —\n",
            "   Mengandung Pidana: True\n",
            "   Mengandung Penganiayaan: True\n",
            "   Ringkasan: Direktori Putusan Mahkamah Agung Republik Indonesia\n",
            "putusan.mahkamahagung.go.id\n",
            "\n",
            "Mahkamah Agung Repu...\n",
            "\n",
            "3. Case ID: 36, Score: 0.7046\n",
            "   No Perkara: 200/PID/2025/PT SMR\n",
            "   Tanggal: 13 Juni 2025\n",
            "   Jenis Perkara: Pidana Umum \n",
            " Pidana Umum  Penganiayaan\n",
            "   Pasal: —\n",
            "   Mengandung Pidana: True\n",
            "   Mengandung Penganiayaan: True\n",
            "   Ringkasan: Direktori Putusan Mahkamah Agung Republik Indonesia\n",
            "putusan.mahkamahagung.go.id\n",
            "\n",
            "Mahkamah Agung Repu...\n",
            "\n",
            "\n",
            "==================================================\n",
            "Query: sengketa kontrak kerja\n",
            "==================================================\n",
            "\n",
            "TF-IDF Results:\n",
            "1. Case ID: 19, Score: 0.0177\n",
            "   No Perkara: 44/Pid.B/2025/PN Blg\n",
            "   Tanggal: 30 April 2025\n",
            "   Jenis Perkara: Pidana Umum \n",
            " Pidana Umum  Penganiayaan\n",
            "   Pasal: —\n",
            "   Mengandung Pidana: True\n",
            "   Mengandung Penganiayaan: True\n",
            "   Ringkasan: Direktori Putusan Mahkamah Agung Republik Indonesia\n",
            "putusan.mahkamahagung.go.id\n",
            "\n",
            "Mahkamah Agung Repu...\n",
            "\n",
            "2. Case ID: 31, Score: 0.0167\n",
            "   No Perkara: 179/PID/2025/PT BDG\n",
            "   Tanggal: 2 Juni 2025\n",
            "   Jenis Perkara: Pidana Umum \n",
            " Pidana Umum  Penganiayaan\n",
            "   Pasal: —\n",
            "   Mengandung Pidana: True\n",
            "   Mengandung Penganiayaan: True\n",
            "   Ringkasan: Direktori Putusan Mahkamah Agung Republik Indonesia\n",
            "putusan.mahkamahagung.go.id\n",
            "\n",
            "Mahkamah Agung Repu...\n",
            "\n",
            "3. Case ID: 12, Score: 0.0101\n",
            "   No Perkara: 109/Pid.B/2025/PN Wtp\n",
            "   Tanggal: 29 April 2025\n",
            "   Jenis Perkara: Pidana Umum \n",
            " Pidana Umum  Penganiayaan\n",
            "   Pasal: —\n",
            "   Mengandung Pidana: True\n",
            "   Mengandung Penganiayaan: True\n",
            "   Ringkasan: Direktori Putusan Mahkamah Agung Republik Indonesia\n",
            "putusan.mahkamahagung.go.id\n",
            "\n",
            "Mahkamah Agung Repu...\n",
            "\n",
            "BERT Results:\n",
            "1. Case ID: 25, Score: 0.6512\n",
            "   No Perkara: 38/Pid.B/2025/PN Bla\n",
            "   Tanggal: 6 Mei 2025\n",
            "   Jenis Perkara: Pidana Umum \n",
            " Pidana Umum  Penganiayaan\n",
            "   Pasal: —\n",
            "   Mengandung Pidana: True\n",
            "   Mengandung Penganiayaan: True\n",
            "   Ringkasan: Direktori Putusan Mahkamah Agung Republik Indonesia\n",
            "putusan.mahkamahagung.go.id\n",
            "\n",
            "Mahkamah Agung Repu...\n",
            "\n",
            "2. Case ID: 7, Score: 0.6510\n",
            "   No Perkara: 46/Pid.B/2025/PN Bla\n",
            "   Tanggal: 19 Mei 2025\n",
            "   Jenis Perkara: Pidana Umum \n",
            " Pidana Umum  Penganiayaan\n",
            "   Pasal: —\n",
            "   Mengandung Pidana: True\n",
            "   Mengandung Penganiayaan: True\n",
            "   Ringkasan: Direktori Putusan Mahkamah Agung Republik Indonesia\n",
            "putusan.mahkamahagung.go.id\n",
            "\n",
            "Mahkamah Agung Repu...\n",
            "\n",
            "3. Case ID: 29, Score: 0.6382\n",
            "   No Perkara: 23/Pid.B/2025/PN Nla\n",
            "   Tanggal: 8 Mei 2025\n",
            "   Jenis Perkara: Pidana Umum \n",
            " Pidana Umum  Penganiayaan\n",
            "   Pasal: —\n",
            "   Mengandung Pidana: True\n",
            "   Mengandung Penganiayaan: True\n",
            "   Ringkasan: Direktori Putusan Mahkamah Agung Republik Indonesia\n",
            "putusan.mahkamahagung.go.id\n",
            "\n",
            "Mahkamah Agung Repu...\n",
            "\n",
            "\n",
            "============================================================\n",
            "=== Case Retrieval System Setup Complete ===\n",
            "============================================================\n",
            "System summary saved to /content/drive/MyDrive/SEMESTER 6/Penalaran Komputer/UAS_Penalaran Komputer/data/eval/system_summary.json\n"
          ]
        }
      ]
    }
  ]
}